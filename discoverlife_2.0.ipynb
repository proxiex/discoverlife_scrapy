{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Settings for notebook\n",
    "from IPython.core.interactiveshell import InteractiveShell\n",
    "InteractiveShell.ast_node_interactivity = \"all\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "try:\n",
    "    import scrapy\n",
    "except:\n",
    "    !pip install scrapy\n",
    "    import scrapy\n",
    "from scrapy.crawler import CrawlerProcess"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define the fields for your item here\n",
    "import scrapy\n",
    "\n",
    "\n",
    "class DiscoverlifeItem(scrapy.Item):\n",
    "    biological_name = scrapy.Field()\n",
    "    common_name = scrapy.Field()\n",
    "    latitude = scrapy.Field()\n",
    "    longitude = scrapy.Field()\n",
    "    date = scrapy.Field()\n",
    "    image_urls = scrapy.Field()\n",
    "    image_paths = scrapy.Field()\n",
    "    url = scrapy.Field()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define pip line\n",
    "import scrapy\n",
    "from scrapy.pipelines.images import ImagesPipeline\n",
    "from scrapy.exceptions import DropItem\n",
    "\n",
    "class MyImagesPipeline(ImagesPipeline):\n",
    "\n",
    "    def get_media_requests(self, item, info):\n",
    "        for image_url in item['image_urls']:\n",
    "            yield scrapy.Request(image_url)\n",
    "\n",
    "    def item_completed(self, results, item, info):\n",
    "        image_paths = [x['path'] for ok, x in results if ok]\n",
    "        if not image_paths:\n",
    "            raise DropItem(\"Item contains no images\")\n",
    "        item['image_paths'] = 'images_2.0/' + image_paths[0]\n",
    "        return item\n",
    "\n",
    "class DropNullBioNm(object):\n",
    "    def process_item(self, item, spider):\n",
    "        if item['biological_name'] == None:\n",
    "            raise DropItem(\"Item empty %s\" % item)\n",
    "        else:\n",
    "            return item\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import scrapy\n",
    "import logging\n",
    "\n",
    "\n",
    "class DiscoverlifeSpider(scrapy.Spider):\n",
    "    name = \"discoverlife\"\n",
    "    custom_settings = {\n",
    "        'LOG_LEVEL': logging.WARNING,\n",
    "        'ITEM_PIPELINES': {\n",
    "            '__main__.DropNullBioNm': 1, # pipline to check if bioname present\n",
    "            '__main__.MyImagesPipeline': 2 # pipline to download images from scraped site\n",
    "        },\n",
    "        'FEED_FORMAT':'csv',                                 \n",
    "        'FEED_URI': 'discoverlife_2.0.csv',\n",
    "        'USER_AGENT': 'Mozilla/4.0 (compatible; MSIE 7.0; Windows NT 5.1)',\n",
    "        'IMAGES_STORE': 'images_2.0',\n",
    "        'DOWNLOAD_TIMEOUT': 672,\n",
    "    }\n",
    "    \n",
    "    start_urls = [\"https://www.discoverlife.org/moth/data/table2_33.9_-83.3.html\"]\n",
    "\n",
    "    def parse(self, response):\n",
    "        # get photo num summary from tables\n",
    "        photo_num  = response.xpath('//table[@border=\"1\"]/tr[@bgcolor]/td[@bgcolor and @align=\"right\"]/text()').extract()\n",
    "        # fetch links to each species\n",
    "        link = response.xpath('//table[@border=\"1\"]/tr[@bgcolor]/td[1]/a/@href').extract()\n",
    "       \n",
    "        for i, num in enumerate(photo_num):\n",
    "            num = int(num.split('\\xa0')[0].replace(',', ''))\n",
    "            # check if total is less than 100, else collect and follow link to species list\n",
    "            if num > 99:\n",
    "                moth_name = link[i].split('=')[1] # extract species name from link\n",
    "                details_link = f'https://www.discoverlife.org/mp/20p?&res=640&selected=1&name={moth_name}&see=name&xml=Moth;'\n",
    "                yield response.follow(details_link, callback=self.get_links)\n",
    "    \n",
    "    def get_links(self, response):\n",
    "        # get links to species details page and follow\n",
    "        for url in response.xpath('//td[@valign=\"top\"]/a[2]/@href').extract():\n",
    "            yield response.follow('https://www.discoverlife.org'+url, callback=self.paser_details)\n",
    "\n",
    "\n",
    "    def paser_details(self, response):\n",
    "        try:\n",
    "            item = DiscoverlifeItem()\n",
    "            # creating xpath defination to extract individual table data items.\n",
    "            xpath = '//table/tr[td/b=\"{}\"]/td/following-sibling::td[1]/text()'.format\n",
    "            bio = response.xpath('//table/tr/td/a/text()').extract()\n",
    "\n",
    "            item['image_urls'] = ['https://www.discoverlife.org' + response.xpath('//div[@align=\"center\"]/a[1]/img/@src').extract()[0]]\n",
    "            item['common_name'] = response.xpath(xpath('title')).extract()[1].split(',')[1].strip()\n",
    "            item['latitude'] = response.xpath(xpath('latitude')).extract()[0]\n",
    "            item['longitude'] = response.xpath(xpath('longitude')).extract()[0]\n",
    "            item['date'] = response.xpath(xpath('date1 yyyymmdd')).extract()[0]\n",
    "            item['biological_name'] = bio[0] if len(bio) > 1 else None\n",
    "            item['url'] = response.url\n",
    "\n",
    "            yield item\n",
    "        except Exception as e:\n",
    "            print('An Error occured :: ', e, request.url)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing.....\n"
     ]
    }
   ],
   "source": [
    "# from scrapy.crawler import CrawlerProcess\n",
    "import scrapy.crawler as crawler\n",
    "from multiprocessing import Process, Queue\n",
    "from twisted.internet import reactor\n",
    "\n",
    "def run_spider(spider):\n",
    "    print('Processing.....')\n",
    "    def f(q):\n",
    "        try:\n",
    "            runner = crawler.CrawlerRunner()\n",
    "            deferred = runner.crawl(spider)\n",
    "            deferred.addBoth(lambda _: reactor.stop())\n",
    "            reactor.run()\n",
    "            q.put(None)\n",
    "        except Exception as e:\n",
    "            q.put(e)\n",
    "\n",
    "    q = Queue()\n",
    "    p = Process(target=f, args=(q,))\n",
    "    p.start()\n",
    "    result = q.get()\n",
    "    p.join()\n",
    "\n",
    "    if result is not None:\n",
    "        raise result\n",
    "    print('Done!')\n",
    "\n",
    "\n",
    "run_spider(DiscoverlifeSpider)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}